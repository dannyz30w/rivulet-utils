{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Rivulet: U. S. Geological Survey Water Quality APIs\n",
    "_by Michelle H Wilkerson_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Purpose of this Notebook\n",
    "\n",
    "This notebook was developed as part of NSF Grant 2445609 to support accessing and processing public data for middle and high school classroom activities. It's written to be relatively accessible to beginners, but if you have not interacted with computational notebooks or python before you may find navigating this tool difficult. (Check out the Show Your Work project for a gentle introduction to computational notebooks for educators!)\n",
    "\n",
    "Our project is focused on supporting data analysis and mechanistic reasoning in science education. In other words, we want students to learn how data provides information about _how scientific mechanisms work_, and how understanding scientific mechanisms can help them to _explain and interpret patterns in data_. This builds on a long history of research on complex systems and agent-based modeling, and more closely connects that work to current expansions of data analysis across subjects.\n",
    "\n",
    "Here, we are focused on Water Quality as a phenomenon. While most students understand that poor Water Quality can impact health, they may not know what sorts of pollutants impact water quality, and what kinds of events or conditions lead to reductions in water quality.\n",
    "\n",
    "This data tool allows users to connect to the United States Geological Survey (USGS) water data APIs, search for water quality data streams in an area of interest, and then provides a collection of ways to filter and search the data to ensure you find datasets that have patterns worth exploring. These kinds of datasets can serve as a launch to examining what WQ is and what are its underlying mechanistic and compositional complexities.\n",
    "\n",
    "You are welcome to modify and adapt this script. You may find the USGS' water data APIs documentation [here]() and [here](https://doi-usgs.github.io/dataretrieval-python/) helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Part 0: What is an API? (Click to expand...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Describe APIs, how they work, how common they are in data science, why they are useful for educators and educational researchers who do data science education work to know about. Describe the risks and concerns about APIs and using them to source data that students will interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Part I: Connecting with USGS Water Data APIs\n",
    "\n",
    "The USGS has developed a Python library (unhelpfully but impressively called `dataretrieval`) to help people access and fetch hydrological data from several different water-related data services.\n",
    "\n",
    "You can sign up for an API key [at this site](https://api.waterdata.usgs.gov/signup). Once you receive it, replace the DEMO_KEY below with your unique API key. Do not share your key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dataretrieval\n",
    "\n",
    "API_KEY = \"DEMO_KEY\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Not sure this is important yet, but the docs say that if you want data after March 2024 you want to specify `legacy=False`. See [here](https://github.com/DOI-USGS/dataretrieval-python#:~:text=%E2%9A%A0%EF%B8%8F,the%20wqp%20module.) and [here](https://doi-usgs.github.io/dataRetrieval/articles/Status.html#:~:text=Discrete%20Data,non%2DUSGS%20data) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataretrieval.nwis as nwis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Part 2: Specifying a Location and Time Period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Let's create a bounding box to indicate the region we are interested in. We will then filter our queries to focus only on monitoring sites within the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT HERE: Define a bounding box around your\n",
    "# target region. If it is densely populated, we suggest\n",
    "# you start with a bounding box that is only one degree\n",
    "# in area. \n",
    "\n",
    "min_lat = 37.5 # CHANGE TO YOUR MINIMUM LATITUDE\n",
    "max_lat = 38 # CHANGE TO YOUR MAXIMUM LATITUDE\n",
    "\n",
    "min_long = -122.5 # CHANGE TO YOUR MINIMUM LONGITUDE\n",
    "max_long = -122\n",
    "\n",
    "# this is unnecessary but sort of luxurious. let's map the box to\n",
    "# make sure we're capturing what we want.\n",
    "\n",
    "import folium\n",
    "\n",
    "bbox = [[min_lat, min_long], [max_lat, max_long]]\n",
    "\n",
    "# Calculate the center of the box to position the map\n",
    "map_center = [(bbox[0][0] + bbox[1][0]) / 2, (bbox[0][1] + bbox[1][1]) / 2]\n",
    "\n",
    "# Create a Folium map object\n",
    "m = folium.Map(location=map_center, zoom_start=8)\n",
    "\n",
    "# Add a rectangle for the bounding box to the map\n",
    "folium.Rectangle(\n",
    "    bounds=bbox,\n",
    "    color=\"#ff0000\",        # Red border\n",
    "    fill=True,\n",
    "    fill_color=\"#ff7800\",   # Orange fill\n",
    "    fill_opacity=0.2\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We're gonna fetch the list of sites within the specified bounding box above. \n",
    "\n",
    "Like the AQS team, the NWIS team is awesome and fetch you some beautiful data. For each request, they pass back a tuple (in this case, a two-ple) of dataframe, metadata. So be sure you catch what's returned with that format. Below, since we are requesting sites, we assign the results of the request to the tuple sites, sites_metadata.\n",
    "\n",
    "Note that at the bottom of the output here, you'll get a URL. That's the corresponding GET request that you can put in to get the same data. Helpful for debugging if something's not working as you would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bBox (list): A contiguous range of decimal latitude and longitude.\n",
    "# Starts with the west longitude, then the south latitude, \n",
    "# then the east longitude, and then the north latitude \n",
    "# with each value separated by a comma. \n",
    "# The product of the range of latitude range and longitude cannot exceed 25 degrees. \n",
    "# TODO: Ok I have no idea how to smartly translate from the idea of min/max \n",
    "# (especially considering different hemispheres) to this idea of east west. \n",
    "# Since these are US Services maybe we can ignore the hemisphere question for now...\n",
    "\n",
    "bbox = str(min_long) + \",\" + str(min_lat) + \",\" + str(max_long) + \",\" + str(max_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Whoa! Ok that's a lot of sites. Depending on the phenomena we're interested in, we'll filter this to only what we need.\n",
    "\n",
    "Next, we want to identify a date when we think something interesting is happening (or, a date we're interested in for other reasons). California experienced king tides during Jan 11-13 this year, so let's identify Jan 12 as a target date. \n",
    "\n",
    "Below, we set the target date. We'll use this to see which monitoring stations were active at the time of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT HERE: identify a target date when something interesting\n",
    "# was happening. Below, I define Jan 12, 2025, during king tides in CA.\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "target_date = \"11-11-2024\"\n",
    "target_datetime = datetime.strptime(target_date, \"%m-%d-%Y\")\n",
    "\n",
    "start_date = target_datetime\n",
    "end_date = target_datetime + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The function what_sites() allows us to see which sites were active and recording the things we're interested in during our specified date. Let's use this to filter our list. You can also filter by what's being measured, see the list in [USGS parameter codes](https://help.waterdata.usgs.gov/parameter_cd?group_cd=PHY). There are a ton of different salinity parameters, so let's just look at the ones that the USGS and EPA agree on (and are checked for accuracy):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Part 3: Salinity in Estuaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This first section helps you select datasets that feature information about salinity in bodies of water where salinity behaves in interesting ways. This includes in estuaries where fresh and salt water meet, and in cold areas during winter months, when salt road treatments run off into freshwater sources. \n",
    "\n",
    "These patterns connect to inflection points and understanding tidal dynamics at the interface or fresh and salt water, as the tides flow in and out.\n",
    "\n",
    "Here's the deal. Salinity is measured through conductance. So in NWIS, there are a ton of salinity measures, but the best one to look for is \"specific conductance\", which is a measure of salinity that is also adjusted for temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "estuaries, estuaries_metadata = nwis.what_sites(bBox=bbox,\n",
    "                                        startDt=start_date,\n",
    "                                        endDt=end_date,\n",
    "                                        parameterCd='00095',\n",
    "                                        siteType='ES') #estuaries\n",
    "\n",
    "estuaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Many sites that have been returned in my tests of this code that have this parameter as a column, but only have NaN values. That's frustrating. This seems to correspond to records that have NaNs in the alt_datum_cd column, though eventually someone should come back and check this more thoroughly. For me, now, it's a heuristic based on my eyeballing and playing with SF Bay Area data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "estuaries = estuaries[~estuaries['alt_datum_cd'].isna()]\n",
    "\n",
    "estuaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "YAAS! Ok, for the estuaries, we'll want to see the flows in and out, so we want maybe a week or two of data around the target date. Pick your favorite site from the list above, and plop it into the code below to pull the specific conductance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset our start and end time to a period of 2 weeks surrounding the target date\n",
    "start_date = target_datetime - timedelta(days=4)\n",
    "end_date = target_datetime + timedelta(days=4)\n",
    "\n",
    "# ones that work 374722122155501\n",
    "# 373015122071000\n",
    "\n",
    "sal_data, meta = nwis.get_iv(sites='373015122071000', #site of your choice \n",
    "                           parameterCd='00095',\n",
    "                           start=start_date.strftime(\"%Y-%m-%d\"),\n",
    "                           end=end_date.strftime(\"%Y-%m-%d\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Plot it to be sure we see what we think we'll see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(data=sal_data,x='datetime', y='00095_upper: 25 ft from bed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Now we have a list of only estuaries. Let's filter so that we're only looking at estuaries that "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Part 4: EColi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Ecoli problems occur in water sources after rains, when sewers release bacteria. Let's check it out. Remember that you defined your location bounding box and your target date in Part 2 setup above. Go up and reset these if you have been working through the notebook in order, to identify a time close to a rain, perhaps after a dry spell.\n",
    "\n",
    "For SF Bay area, we're going to look for creeks. We're going to use the target date of Sept 9 because I remember that's when I got all wet biking to campus :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### eventually clean this up so target date is taken care of in \n",
    "# the setup section only but for now I'm just going\n",
    "# to do it here\n",
    "target_date = \"01-04-2023\"\n",
    "target_datetime = datetime.strptime(target_date, \"%m-%d-%Y\")\n",
    "\n",
    "start_date = target_datetime\n",
    "end_date = target_datetime + timedelta(days=1)\n",
    "## end cleanup section\n",
    "\n",
    "# check the parameter here, I went for one that was there and made sense\n",
    "ecoli_data, ec_meta = nwis.what_sites(bBox=bbox,\n",
    "                                      startDt=start_date,\n",
    "                                      endDt=end_date,\n",
    "                                      parameterCd='31625')#, #fecal coliforms\n",
    "                                    #  siteType='ST') #streams\n",
    "\n",
    "ecoli_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Ok just FYI you can search for body parts as an attribute that's available for what's in the water.\n",
    "\n",
    "Note that for e coli data, we use a different module from the one we used in section 3 estuaries. Here, we use a module called WQP. More info [here]('https://code.usgs.gov/water/dataretrieval-python/-/blob/v1.0.2/dataretrieval/wqp.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#about a week surrounding target date\n",
    "start_date = target_datetime - timedelta(days=3)\n",
    "end_date = target_datetime + timedelta(days=3)\n",
    "\n",
    "from dataretrieval import wqp\n",
    "\n",
    "ec_data, ecmeta = wqp.get_results(siteid='11181330', #site of your choice \n",
    "                           pCode='31625,50468,90901',\n",
    "                           startDateLo=start_date.strftime(\"%m-%d-%Y\"),\n",
    "                           startDateHi=end_date.strftime(\"%m-%d-%Y\"))\n",
    "\n",
    "ec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#50468\tE. coli, modified m-TEC method\tMost Probable Number per 100 mL (MPN/100 mL)\n",
    "#90901\tE. coli, unspecified method\tColonies per 100 mL (cfu/100 mL)\n",
    "#31633\tE. coli, m-TEC MF method\tColony Forming Units per 100 mL (cfu/100 mL)\n",
    "#31648\tE. coli, m-TEC MF method\tColonies per 100 mL (cfu/100 mL)\n",
    "\n",
    "E Coli is most likely to be found the sites most likely to show dramatic increases in E. coli after a rain event are smaller, fast-reacting bodies of water in watersheds with significant sources of fecal contamination. These are often called \"flashy\" systems.\n",
    "\n",
    "in order of likelihood of a dramatic effect, check out sites that are:\n",
    "- storm drains\n",
    "- urban creeks and streams\n",
    "- coastal beaches and river mouths\n",
    "- small lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Part 5: Oxygen Dissolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Part 6: Nitrates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Part 7: Lead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "We are grateful to Dr. Lex Van Geen for his insights into pedagogically productive datasets.\n",
    "\n",
    "Hodson, T.O., Hariharan, J.A., Black, S., and Horsburgh, J.S., 2023, dataretrieval (Python): a Python package for discovering and retrieving water data available from U.S. federal hydrologic web services: U.S. Geological Survey software release, https://doi.org/10.5066/P94I5TX3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
